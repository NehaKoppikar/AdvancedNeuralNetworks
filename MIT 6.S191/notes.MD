[Website](http://introtodeeplearning.com/)
<br>
[Code](https://github.com/aamini/introtodeeplearning/)



### Class 1

[Video](https://www.youtube.com/watch?v=5tvmMX8r_OM&t=806s&ab_channel=AlexanderAmini)

Why do we need activation functions?
Reasons:
- Introduce non-linearities into the network
- In the real world, data is almost always non-linear
- It allows the neural network to approximate arbitrarily complex functions

Key Takeaways of how a perceptron works:
1. Take a dot product of your inputs and your weights
1. Add a bias
1. Apply non-linearity

Learning Rate - How fast we want to learn